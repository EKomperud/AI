function [U,U_trace] = CS4300_MDP_value_iteration(S,A,P,R,gamma,...
eta,max_iter)
% CS4300_MDP_value_iteration - compute policy using value iteration
% On input:
%   S (vector): states (1 to n)
%   A (vector): actions (1 to k)
%   P (nxk struct array): transition model
%       (s,a).probs (a vector with n transition probabilities
%       (from s to s_prime, given action a)
%   R (vector): state rewards
%   gamma (float): discount factor
%   eta (float): termination threshold
%   max_iter (int): max number of iterations
% On output:
%   U (vector): state utilities
%   U_trace (iterxn): trace of utility values during iteration
% Call:
%   [U,Ut] = Cs4300_MDP_value_iteration(S,A,P,R,0.999999,0.1,100);
%
%   Set up a driver function, CS_4300_run_value_iteration (see
%   below), which sets up the Markov Decision Problem and calls this
%   function.
%
%   Chapter 17 Russell and Norvig (Table p. 651)
%   [S,A,R,P,U,Ut] = CS4300_run_value_iteration(0.999999,1000)
%
%   U’ = 0.7053 0.6553 0.6114 0.3879 0.7616 0 0.6600 -1.0000
%     0.8116 0.8678 0.9178 1.0000
%
%   Layout:             1
%                       ˆ
%    9 10 11 12         |
%    5  6  7  8     2 <- -> 4
%    1  2  3  4         |
%                       V
%                       3
% Author:
%   Eric Komperud
%   U0844210
%   Fall 2017
%

% (S)tates        = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16];
% (A)ctions       = [1 (Up), 2 (Left), 3 (Down), 4 (Right)];
U = zeros(16,1);
U(3) = -1000;
U(7) = -1000;
U(11) = -1000;
U(16) = 1000;

Up = zeros(16,1);
Up(3) = -1000;
Up(7) = -1000;
Up(11) = -1000;
Up(16) = 1000;

iterations = 0;
U_trace = zeros(max_iter, 16);
Slen = len(S);
Alen = len(A);

while (1)
    U = Up;
    curly_fuck = 0;
    for state = 1:16
        Possible_Utilities = [0,0,0,0];
        for action = 1:4
            v = P(state, action);
            vp = [];
            vi = 1;
            for vs = 1:16
                if v(vs) ~= 0.0
                    vp(vi) = v(vs) * U(vs);
                    vi = vi + 1;
                end
            end
            Possible_Utilities(action) = max(vp);
        end
        Up(state) = R(state) + gamma * (max(Possible_Utilities));  
        Up(3) = -1000;
        Up(7) = -1000;
        Up(11) = -1000;
        Up(16) = 1000;       
        curly_fuck = Max(curly_fuck, Up(1) - U(1));
    end
    U_trace(iterations) = Up;
    if curly_fuck < (eta * (1 - gamma) / gamma)
        return;
    end
    
    iterations = iterations + 1;
    if iterations >= max_iter
        return;
    end
end


end

